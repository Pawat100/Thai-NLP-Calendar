"""
NLP Utilities for Thai Calendar Event Extraction

This module provides functions extracted from THENLP.ipynb for use in the Streamlit app.
Includes NER, POS validation, slot mapping, and date/time parsing.
"""

import spacy
import json
import re
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import pytz
try:
    from pythainlp import normalize
    import dateparser
except ImportError:
    print("Warning: pythainlp or dateparser not installed")

# Constants
TZ = pytz.timezone('Asia/Bangkok')
EVENTS_FILE = "events.json"

# Global NLP model (loaded once)
_nlp_model = None

# =========================
# Normalization Dictionaries (from NLP_PROJECT.ipynb)
# =========================

SLANG_DICT = {
    # ðŸ“… à¸§à¸±à¸™ / à¸§à¸±à¸™à¸—à¸µà¹ˆ
    "à¸žà¸™.": "à¸žà¸£à¸¸à¹ˆà¸‡à¸™à¸µà¹‰",
    "à¸žà¸™": "à¸žà¸£à¸¸à¹ˆà¸‡à¸™à¸µà¹‰",
    "à¸¡à¸°à¸£à¸·à¸™": "à¸§à¸±à¸™à¸–à¸±à¸”à¹„à¸›",
    "à¸¡à¸°à¸¥à¸·à¸™à¸™à¸µà¹‰": "à¸§à¸±à¸™à¸–à¸±à¸”à¹„à¸›",
    "à¸¡à¸°à¸§à¸²à¸™": "à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™",
    
    # â° à¸Šà¹ˆà¸§à¸‡à¹€à¸§à¸¥à¸² (Intent)
    "à¸•à¸­à¸™à¹€à¸Šà¹‰à¸²": "à¸Šà¹ˆà¸§à¸‡à¹€à¸Šà¹‰à¸²",
    "à¹€à¸Šà¹‰à¸²": "à¸Šà¹ˆà¸§à¸‡à¹€à¸Šà¹‰à¸²",
    "à¸•à¸­à¸™à¸ªà¸²à¸¢": "à¸Šà¹ˆà¸§à¸‡à¸ªà¸²à¸¢",
    "à¸ªà¸²à¸¢": "à¸Šà¹ˆà¸§à¸‡à¸ªà¸²à¸¢",
    "à¸•à¸­à¸™à¸šà¹ˆà¸²à¸¢": "à¸Šà¹ˆà¸§à¸‡à¸šà¹ˆà¸²à¸¢",
    "à¸šà¹ˆà¸²à¸¢": "à¸Šà¹ˆà¸§à¸‡à¸šà¹ˆà¸²à¸¢",
    "à¸•à¸­à¸™à¹€à¸¢à¹‡à¸™": "à¸Šà¹ˆà¸§à¸‡à¹€à¸¢à¹‡à¸™",
    "à¹€à¸¢à¹‡à¸™": "à¸Šà¹ˆà¸§à¸‡à¹€à¸¢à¹‡à¸™",
    "à¸•à¸­à¸™à¸„à¹ˆà¸³": "à¸Šà¹ˆà¸§à¸‡à¸„à¹ˆà¸³",
    "à¸„à¹ˆà¸³": "à¸Šà¹ˆà¸§à¸‡à¸„à¹ˆà¸³",
    "à¸•à¸­à¸™à¸”à¸¶à¸": "à¸Šà¹ˆà¸§à¸‡à¸”à¸¶à¸",
    "à¸”à¸¶à¸": "à¸Šà¹ˆà¸§à¸‡à¸”à¸¶à¸",
    
    # ðŸ•’ à¹€à¸§à¸¥à¸²
    "à¹€à¸—à¸µà¹ˆà¸¢à¸‡": "12:00",
    "à¹€à¸—à¸µà¹ˆà¸¢à¸‡à¸„à¸·à¸™": "00:00",
    "à¸šà¹ˆà¸²à¸¢à¹‚à¸¡à¸‡": "13:00",
    "à¸šà¹ˆà¸²à¸¢à¸ªà¸­à¸‡": "14:00",
    "à¸šà¹ˆà¸²à¸¢à¸ªà¸²à¸¡": "15:00",
    "à¸šà¹ˆà¸²à¸¢à¸ªà¸µà¹ˆ": "16:00",
    "à¸šà¹ˆà¸²à¸¢à¸«à¹‰à¸²": "17:00",
    "à¸«à¸à¹‚à¸¡à¸‡à¹€à¸¢à¹‡à¸™": "18:00",
    "à¸«à¸™à¸¶à¹ˆà¸‡à¸—à¸¸à¹ˆà¸¡": "19:00",
    "à¸ªà¸­à¸‡à¸—à¸¸à¹ˆà¸¡": "20:00",
    "à¸ªà¸²à¸¡à¸—à¸¸à¹ˆà¸¡": "21:00",
    
    # ðŸ‘¤ à¸šà¸¸à¸„à¸„à¸¥
    "à¸ˆà¸²à¸£": "à¸­à¸²à¸ˆà¸²à¸£à¸¢à¹Œ",
    "à¸­à¸ˆ": "à¸­à¸²à¸ˆà¸²à¸£à¸¢à¹Œ",
    "à¸­.": "à¸­à¸²à¸ˆà¸²à¸£à¸¢à¹Œ",
    "à¸šà¸­à¸ª": "à¸œà¸¹à¹‰à¸šà¸±à¸‡à¸„à¸±à¸šà¸šà¸±à¸à¸Šà¸²",
    "à¸«à¸±à¸§à¸«à¸™à¹‰à¸²": "à¸œà¸¹à¹‰à¸šà¸±à¸‡à¸„à¸±à¸šà¸šà¸±à¸à¸Šà¸²",
    
    # ðŸ—£ï¸ à¸à¸£à¸´à¸¢à¸²
    "à¸™à¸±à¸”à¹€à¸ˆà¸­": "à¸™à¸±à¸”à¸žà¸š",
    "à¹€à¸ˆà¸­à¸à¸±à¸™": "à¸žà¸š",
    "à¹„à¸›à¸«à¸²": "à¹„à¸›à¸žà¸š",
    "à¹€à¸‚à¹‰à¸²à¹„à¸›à¸«à¸²": "à¹„à¸›à¸žà¸š",
    "à¸„à¸¸à¸¢à¸‡à¸²à¸™": "à¸›à¸£à¸°à¸Šà¸¸à¸¡",
    "à¹€à¸‚à¹‰à¸²à¹„à¸›à¸„à¸¸à¸¢": "à¸›à¸£à¸°à¸Šà¸¸à¸¡",
    "à¹€à¸¥à¸·à¹ˆà¸­à¸™à¸™à¸±à¸”": "à¹€à¸¥à¸·à¹ˆà¸­à¸™",
    "à¸¢à¸à¹€à¸¥à¸´à¸à¸™à¸±à¸”": "à¸¢à¸à¹€à¸¥à¸´à¸",
    
    # ðŸ« à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆ
    "à¸¡à¸—à¸£.": "à¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢",
    "à¸¡à¸—à¸£": "à¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢",
    "à¸¡à¸­": "à¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢",
    "à¸¡à¸«à¸²à¸¥à¸±à¸š": "à¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢",
    "à¸£à¸²à¸Šà¸¡à¸‡à¸„à¸¥à¸žà¸£à¸°à¸™à¸„à¸£": "à¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢",
    "rmutp": "à¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢",
    "à¸•à¸¶à¸à¹€à¸£à¸µà¸¢à¸™": "à¸­à¸²à¸„à¸²à¸£à¹€à¸£à¸µà¸¢à¸™",
    "à¸•à¸¶à¸": "à¸­à¸²à¸„à¸²à¸£",
    
    # ðŸŽ“ à¸„à¸“à¸°
    "à¸„à¸“à¸°à¸§à¸´à¸¨à¸§à¸°": "à¸„à¸“à¸°à¸§à¸´à¸¨à¸§à¸à¸£à¸£à¸¡à¸¨à¸²à¸ªà¸•à¸£à¹Œ",
    "à¸§à¸´à¸¨à¸§à¸°": "à¸„à¸“à¸°à¸§à¸´à¸¨à¸§à¸à¸£à¸£à¸¡à¸¨à¸²à¸ªà¸•à¸£à¹Œ",
    "à¸„à¸“à¸°à¸šà¸£à¸´à¸«à¸²à¸£": "à¸„à¸“à¸°à¸šà¸£à¸´à¸«à¸²à¸£à¸˜à¸¸à¸£à¸à¸´à¸ˆ",
    "à¸šà¸£à¸´à¸«à¸²à¸£": "à¸„à¸“à¸°à¸šà¸£à¸´à¸«à¸²à¸£à¸˜à¸¸à¸£à¸à¸´à¸ˆ",
    "à¸„à¸“à¸°à¹„à¸­à¸—à¸µ": "à¸„à¸“à¸°à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µà¸ªà¸²à¸£à¸ªà¸™à¹€à¸—à¸¨",
    "à¹„à¸­à¸—à¸µ": "à¸„à¸“à¸°à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µà¸ªà¸²à¸£à¸ªà¸™à¹€à¸—à¸¨",
}

LOANWORD_DICT = {
    # à¸à¸´à¸ˆà¸à¸£à¸£à¸¡
    "video call": "à¹‚à¸—à¸£",
    "google meet": "à¸­à¸­à¸™à¹„à¸¥à¸™à¹Œ",
    "ms teams": "à¸­à¸­à¸™à¹„à¸¥à¸™à¹Œ",
    "meeting": "à¸›à¸£à¸°à¸Šà¸¸à¸¡",
    "meet": "à¸›à¸£à¸°à¸Šà¸¸à¸¡",
    "mtg": "à¸›à¸£à¸°à¸Šà¸¸à¸¡",
    "meetup": "à¸›à¸£à¸°à¸Šà¸¸à¸¡",
    "briefing": "à¸Šà¸µà¹‰à¹à¸ˆà¸‡",
    "brief": "à¸Šà¸µà¹‰à¹à¸ˆà¸‡",
    "presentation": "à¸™à¸³à¹€à¸ªà¸™à¸­",
    "present": "à¸™à¸³à¹€à¸ªà¸™à¸­",
    "review": "à¸—à¸šà¸—à¸§à¸™",
    "report": "à¸£à¸²à¸¢à¸‡à¸²à¸™",
    "update": "à¸­à¸±à¸›à¹€à¸”à¸•",
    
    # à¹€à¸§à¸¥à¸²
    "tomorrow": "à¸žà¸£à¸¸à¹ˆà¸‡à¸™à¸µà¹‰",
    "today": "à¸§à¸±à¸™à¸™à¸µà¹‰",
    "tonight": "à¸„à¸·à¸™à¸™à¸µà¹‰",
    "morning": "à¸Šà¹ˆà¸§à¸‡à¹€à¸Šà¹‰à¸²",
    "afternoon": "à¸Šà¹ˆà¸§à¸‡à¸šà¹ˆà¸²à¸¢",
    "evening": "à¸Šà¹ˆà¸§à¸‡à¹€à¸¢à¹‡à¸™",
    
    # à¸­à¸­à¸™à¹„à¸¥à¸™à¹Œ
    "zoom": "à¸­à¸­à¸™à¹„à¸¥à¸™à¹Œ",
    "online": "à¸­à¸­à¸™à¹„à¸¥à¸™à¹Œ",
}

SPLIT_WORD_CORRECTION = {
    ("à¸¡à¸«à¸²", "à¸¥à¸±à¸š"): "à¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢",
    ("à¸¡à¸«à¸²", "à¸¥à¸±à¸¢"): "à¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢",
    ("à¸§à¸´à¸¨", "à¸§à¸°"): "à¸„à¸“à¸°à¸§à¸´à¸¨à¸§à¸à¸£à¸£à¸¡à¸¨à¸²à¸ªà¸•à¸£à¹Œ",
    ("à¸§à¸´à¸¨à¸§", "à¸°"): "à¸„à¸“à¸°à¸§à¸´à¸¨à¸§à¸à¸£à¸£à¸¡à¸¨à¸²à¸ªà¸•à¸£à¹Œ",
    ("à¹‚à¸£à¸‡", "à¸šà¸²à¸¥"): "à¹‚à¸£à¸‡à¸žà¸¢à¸²à¸šà¸²à¸¥",
    ("à¸•à¸­à¸™", "à¹€à¸Šà¹‰à¸²"): "à¸Šà¹ˆà¸§à¸‡à¹€à¸Šà¹‰à¸²",
    ("à¸•à¸­à¸™", "à¸ªà¸²à¸¢"): "à¸Šà¹ˆà¸§à¸‡à¸ªà¸²à¸¢",
    ("à¸•à¸­à¸™", "à¸šà¹ˆà¸²à¸¢"): "à¸Šà¹ˆà¸§à¸‡à¸šà¹ˆà¸²à¸¢",
    ("à¸•à¸­à¸™", "à¹€à¸¢à¹‡à¸™"): "à¸Šà¹ˆà¸§à¸‡à¹€à¸¢à¹‡à¸™",
}


def load_ner_model(model_path: str = "./my_ner_model"):
    """
    Load spaCy NER model
    """
    global _nlp_model
    
    if _nlp_model is not None:
        return _nlp_model
    
    try:
        _nlp_model = spacy.load(model_path)
        print(f"âœ“ Loaded model from {model_path}")
    except OSError:
        print(f"âš  Model not found at {model_path}, creating blank model")
        _nlp_model = spacy.blank("th")
        ner = _nlp_model.add_pipe("ner")
        for label in ["DATE", "TIME", "ACTIVITY", "EVENT", "PERSON", "LOCATION"]:
            ner.add_label(label)
    
    return _nlp_model


def normalize_thai_text(text: str) -> str:
    """
    Normalize Thai text using pythainlp and custom dictionaries
    Applies: Unicode normalization, slang normalization, loanword conversion
    """
    # Step 1: Basic unicode normalization
    try:
        text = normalize(text)
    except:
        pass
    
    # Step 2: Lowercase for matching
    text_lower = text.lower()
    
    # Step 3: Apply loanword dictionary (case-insensitive)
    for loanword, thai_word in LOANWORD_DICT.items():
        text_lower = text_lower.replace(loanword.lower(), thai_word)
    
    # Step 4: Apply slang dictionary
    for slang, formal in SLANG_DICT.items():
        text_lower = text_lower.replace(slang, formal)
    
    # Step 5: Whitespace cleanup
    text_lower = re.sub(r'\s+', ' ', text_lower).strip()
    
    return text_lower


def get_current_datetime():
    """Get current datetime in Bangkok timezone"""
    return datetime.now(TZ)


def parse_thai_date(date_str: str, reference_date: Optional[datetime] = None) -> Optional[str]:
    """
    Parse Thai date expressions to YYYY-MM-DD format
    Enhanced to handle complex formats like "Monday 10 Jan 69"
    """
    if not date_str:
        return None
    
    if reference_date is None:
        reference_date = get_current_datetime()
    
    date_str = date_str.strip().lower()
    
    # Thai relative dates
    thai_relative_dates = {
        'à¸§à¸±à¸™à¸™à¸µà¹‰': 0,
        'à¸žà¸£à¸¸à¹ˆà¸‡à¸™à¸µà¹‰': 1,
        'à¸¡à¸°à¸£à¸·à¸™à¸™à¸µà¹‰': 2,
        'à¸§à¸±à¸™à¸–à¸±à¸”à¹„à¸›': 2,
        'à¹€ à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™': -1,
        'à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™à¸™à¸µà¹‰': -1,
        'à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™à¸‹à¸·à¸™': -2,
        'à¸§à¸²à¸™à¸™à¸µà¹‰': -1,
    }
    
    for thai_word, days in thai_relative_dates.items():
        if thai_word in date_str:
            target_date = reference_date + timedelta(days=days)
            return target_date.strftime('%Y-%m-%d')
    
    # Thai months
    thai_months = {
        'à¸¡à¸à¸£à¸²à¸„à¸¡': 1, 'à¸¡.à¸„.': 1, 'à¸à¸¸à¸¡à¸ à¸²à¸žà¸±à¸™à¸˜à¹Œ': 2, 'à¸.à¸ž.': 2,
        'à¸¡à¸µà¸™à¸²à¸„à¸¡': 3, 'à¸¡à¸µ.à¸„.': 3, 'à¹€à¸¡à¸©à¸²à¸¢à¸™': 4, 'à¹€à¸¡.à¸¢.': 4,
        'à¸žà¸¤à¸©à¸ à¸²à¸„à¸¡': 5, 'à¸ž.à¸„.': 5, 'à¸¡à¸´à¸–à¸¸à¸™à¸²à¸¢à¸™': 6, 'à¸¡à¸´.à¸¢.': 6,
        'à¸à¸£à¸à¸Žà¸²à¸„à¸¡': 7, 'à¸.à¸„.': 7, 'à¸ªà¸´à¸‡à¸«à¸²à¸„à¸¡': 8, 'à¸ª.à¸„.': 8,
        'à¸à¸±à¸™à¸¢à¸²à¸¢à¸™': 9, 'à¸.à¸¢.': 9, 'à¸•à¸¸à¸¥à¸²à¸„à¸¡': 10, 'à¸•.à¸„.': 10,
        'à¸žà¸¤à¸¨à¸ˆà¸´à¸à¸²à¸¢à¸™': 11, 'à¸ž.à¸¢.': 11, 'à¸˜à¸±à¸™à¸§à¸²à¸„à¸¡': 12, 'à¸˜.à¸„.': 12,
    }
    
    # Extract all numbers
    numbers = re.findall(r'\d+', date_str)
    
    # Try to parse with month (higher priority than weekday alone)
    for thai_month, month_num in thai_months.items():
        if thai_month in date_str:
            day = int(numbers[0]) if numbers else 1
            year = reference_date.year
            
            # Check if year is also specified (2-digit or 4-digit)
            if len(numbers) >= 2:
                year_candidate = int(numbers[1])
                # Handle 2-digit year (assume 2500+ for Buddhist era, 20xx for Christian era)
                if year_candidate < 100:
                    if year_candidate >= 50:
                        year = 2000 + year_candidate
                    else:
                        year = 2500 + year_candidate  # Buddhist era
                elif year_candidate > 2500:  # Buddhist year
                    year = year_candidate - 543
                else:
                    year = year_candidate
            
            try:
                target_date = datetime(year, month_num, day, tzinfo=TZ)
                if target_date < reference_date:
                    target_date = datetime(year + 1, month_num, day, tzinfo=TZ)
                return target_date.strftime('%Y-%m-%d')
            except ValueError:
                pass
    
    # Thai weekdays (fallback if no month specified)
    thai_weekdays = {
        'à¸ˆà¸±à¸™à¸—à¸£à¹Œ': 0, 'à¸­à¸±à¸‡à¸„à¸²à¸£': 1, 'à¸žà¸¸à¸˜': 2, 'à¸žà¸¤à¸«à¸±à¸ªà¸šà¸”à¸µ': 3,
        'à¸žà¸¤à¸«à¸±à¸ª': 3, 'à¸¨à¸¸à¸à¸£à¹Œ': 4, 'à¹€à¸ªà¸²à¸£à¹Œ': 5, 'à¸­à¸²à¸—à¸´à¸•à¸¢à¹Œ': 6,
    }
    
    for thai_day, weekday in thai_weekdays.items():
        if thai_day in date_str:
            current_weekday = reference_date.weekday()
            days_ahead = weekday - current_weekday
            if days_ahead <= 0:
                days_ahead += 7
            target_date = reference_date + timedelta(days=days_ahead)
            return target_date.strftime('%Y-%m-%d')
    
    # Fallback to dateparser
    try:
        parsed = dateparser.parse(
            date_str,
            languages=['th', 'en'],
            settings={'TIMEZONE': 'Asia/Bangkok', 'RELATIVE_BASE': reference_date.replace(tzinfo=None)}
        )
        if parsed:
            return parsed.strftime('%Y-%m-%d')
    except:
        pass
    
    return None


def parse_thai_time(time_str: str) -> Optional[str]:
    """
    Parse Thai time expressions to HH:MM format
    Enhanced to handle time ranges - extracts the START time
    Supports both : and . as separators (10:00 or 10.00)
    Examples: "10:00â€“12:00" -> "10:00", "10.00-12.00" -> "10:00"
    """
    if not time_str:
        return None
    
    time_str = time_str.strip().lower()
    
    # Handle time ranges - extract first time only  
    # Common separators: â€“ (en-dash), - (hyphen), ~, à¸–à¸¶à¸‡
    for separator in ['â€“', '-', '~', 'à¸–à¸¶à¸‡', 'to']:
        if separator in time_str:
            parts = time_str.split(separator)
            if parts:
                time_str = parts[0].strip()  # Take only the start time
            break
    
    # Find time patterns - support both : and . as separators
    # Pattern 1: HH:MM or HH.MM
    time_pattern = re.findall(r'(\d{1,2})[:.](\d{2})', time_str)
    if time_pattern:
        hour, minute = int(time_pattern[0][0]), int(time_pattern[0][1])
        if 0 <= hour <= 23 and 0 <= minute <= 59:
            return f"{hour:02d}:{minute:02d}"
    
    # Extract numbers for non-formatted times
    numbers = re.findall(r'\d+', time_str)
    
    hour = 0
    minute = 0
    
    if numbers:
        hour = int(numbers[0])
        if len(numbers) > 1:
            minute = int(numbers[1])
    
    # Thai time period adjustments
    if any(word in time_str for word in ['à¸šà¹ˆà¸²à¸¢', 'à¹€à¸¢à¹‡à¸™', 'à¸„à¹ˆà¸³']):
        if hour < 12:
            hour += 12
    
    if 'à¸„à¸£à¸¶à¹ˆà¸‡' in time_str:
        minute = 30
    
    if 0 <= hour <= 23 and 0 <= minute <= 59:
        return f"{hour:02d}:{minute:02d}"
    
    return None


def extract_entities_with_pos(text: str, nlp_model=None) -> List[Tuple[str, str, str]]:
    """Extract entities with POS tags for validation"""
    if nlp_model is None:
        nlp_model = load_ner_model()
    
    text = normalize_thai_text(text)
    doc = nlp_model(text)
    
    results = []
    for ent in doc.ents:
        pos_tags = [token.pos_ for token in ent]
        main_pos = pos_tags[0] if pos_tags else "UNKNOWN"
        results.append((ent.text, ent.label_, main_pos))
    
    return results


def split_by_separators(text: str) -> List[str]:
    """
    Split text into multiple event segments using common separators.
    
    Separators include:
    - à¹à¸¥à¸°, à¹à¸¥à¹‰à¸§, à¸à¸±à¸š (Thai 'and', 'then', 'with')
    - and, then (English)
    - Commas, semicolons, slashes
    
    Returns list of text segments
    """
    if not text:
        return []
    
    # Define separator patterns (order matters!)
    separators = [
        r'\s+à¹à¸¥à¸°\s+',      # Thai 'and'
        r'\s+à¹à¸¥à¹‰à¸§\s+',     # Thai 'then'  
        r'\s+à¹à¸¥à¹‰à¸§à¸à¹‡\s+',   # Thai 'and then'
        r'\s+à¸žà¸£à¹‰à¸­à¸¡\s+',    # Thai 'along with'
        r'\s+,\s*à¹à¸¥à¸°\s+',  # ', and'
        r'\s+;\s*',        # semicolon
        r'\s+/\s+',        # slash separator
        r'\s*,\s+(?=.{10,})', # comma (but only if followed by substantial text)
        r'\s+and\s+',      # English 'and'
        r'\s+then\s+',     # English 'then'
    ]
    
    # Combine all separators into one pattern
    combined_pattern = '|'.join(f'({sep})' for sep in separators)
    
    # Split text
    segments = re.split(combined_pattern, text, flags=re.IGNORECASE)
    
    # Filter out the separator matches themselves and empty strings
    segments = [seg.strip() for i, seg in enumerate(segments) 
                if i % 2 == 0 and seg and seg.strip()]
    
    return segments if segments else [text]


def extract_multiple_events(text: str, nlp_model=None) -> List[Dict[str, any]]:
    """
    Split text by separators and extract multiple events.
    
    Example:
        Input: "à¸›à¸£à¸°à¸Šà¸¸à¸¡à¸§à¸±à¸™à¸ˆà¸±à¸™à¸—à¸£à¹Œ 10 à¹‚à¸¡à¸‡ à¹à¸¥à¸°à¸ªà¹ˆà¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸žà¸£à¸¸à¹ˆà¸‡à¸™à¸µà¹‰"
        Output: [
            {date: '2026-02-10', time: '10:00', description: 'à¸›à¸£à¸°à¸Šà¸¸à¸¡', ...},
            {date: '2026-02-11', time: None, description: 'à¸ªà¹ˆà¸‡à¹€à¸­à¸à¸ªà¸²à¸£', ...}
        ]
    """
    # Split into segments
    segments = split_by_separators(text)
    
    # Process each segment
    events = []
    for segment in segments:
        slots = extract_slots(segment, nlp_model)
        
        # Only add if it has at least a description or date
        if slots.get('description') or slots.get('date'):
            # Add original segment as raw_text
            slots['raw_text'] = segment
            events.append(slots)
    
    # If no events extracted, return single event from full text
    if not events:
        return [extract_slots(text, nlp_model)]
    
    return events


def extract_slots(text: str, nlp_model=None) -> Dict[str, any]:
    """
    Extract calendar event slots using HYBRID approach:
    1. Rule-based extraction for DATE and TIME (always works)
    2. NER for ACTIVITY, PERSON, LOCATION (if model is trained)
    
    This ensures basic functionality even without a trained model!
    """
    # Normalize text first
    normalized_text = normalize_thai_text(text)
    
    slots = {
        'date': None,
        'time': None,
        'description': None,
        'attendees': [],
        'location': None,
        'raw_text': text
    }
    
    # STEP 1: Rule-based DATE extraction (works without model!)
    # Try to parse date from the original text
    date_parsed = parse_thai_date(normalized_text)
    if date_parsed:
        slots['date'] = date_parsed
    
    # STEP 2: Rule-based TIME extraction (works without model!)
    # Look for time patterns in text
    time_parsed = parse_thai_time(normalized_text)
    if time_parsed:
        slots['time'] = time_parsed
    
    # STEP 3: Try NER for ACTIVITY, PERSON, LOCATION (if model available)
    try:
        entities = extract_entities_with_pos(normalized_text, nlp_model)
        
        for ent_text, label, pos in entities:
            # Only use NER for activity, person, location
            # (Date/time already handled by rules)
            if label in ['ACTIVITY', 'EVENT'] and pos in ['VERB', 'NOUN', 'PROPN', 'UNKNOWN']:
                if not slots['description']:
                    slots['description'] = ent_text
                else:
                    slots['description'] += f", {ent_text}"
            
            elif label == 'PERSON' and pos in ['PROPN', 'NOUN', 'UNKNOWN']:
                slots['attendees'].append(ent_text)
            
            elif label == 'LOCATION' and pos in ['PROPN', 'NOUN', 'UNKNOWN']:
                slots['location'] = ent_text
    except Exception as e:
        # NER failed (model not trained?) - that's OK, we have dates/times from rules
        print(f"NER extraction failed (this is OK if model isn't trained): {e}")
    
    # STEP 4: Fallback - pattern-based extraction for person and location
    if not slots['description']:
        # List of common activity keywords
        activity_keywords = [
            # Meetings & work
            'à¸›à¸£à¸°à¸Šà¸¸à¸¡', 'meeting', 'à¸™à¸±à¸”', 'à¹€à¸ˆà¸­', 'à¸žà¸š',
            'à¹€à¸£à¸µà¸¢à¸™', 'à¸ªà¸­à¸š', 'à¸™à¸³à¹€à¸ªà¸™à¸­', 'presentation',
            'à¸ªà¸±à¸¡à¸¡à¸™à¸²', 'workshop', 'à¸ªà¹ˆà¸‡à¸‡à¸²à¸™', 'à¸£à¸²à¸¢à¸‡à¸²à¸™',
            
            # Food & dining
            'à¸à¸´à¸™à¸‚à¹‰à¸²à¸§', 'à¸à¸´à¸™à¸­à¸²à¸«à¸²à¸£', 'à¸—à¸²à¸™à¸‚à¹‰à¸²à¸§', 'à¸—à¸²à¸™à¸­à¸²à¸«à¸²à¸£',
            'à¸­à¸²à¸«à¸²à¸£', 'à¸¡à¸·à¹‰à¸­', 'à¹€à¸¥à¸µà¹‰à¸¢à¸‡', 'à¸”à¸´à¸™à¹€à¸™à¸­à¸£à¹Œ',
            
            # Social activities
            'à¹€à¸—à¸µà¹ˆà¸¢à¸§', 'à¹„à¸›à¹€à¸—à¸µà¹ˆà¸¢à¸§', 'à¹„à¸›à¹€à¸”à¸´à¸™', 'à¸Šà¹‰à¸­à¸›à¸›à¸´à¹‰à¸‡', 'à¸”à¸¹à¸«à¸™à¸±à¸‡',
            'à¸”à¸¹à¸„à¸­à¸™à¹€à¸ªà¸´à¸£à¹Œà¸•', 'à¸‡à¸²à¸™à¸›à¸²à¸£à¹Œà¸•à¸µà¹‰', 'à¸›à¸²à¸£à¹Œà¸•à¸µà¹‰',
            
            # Health & wellness
            'à¸«à¸¡à¸­', 'à¸„à¸¥à¸´à¸™à¸´à¸', 'à¸£à¸±à¸à¸©à¸²', 'à¸•à¸£à¸§à¸ˆ', 'à¹‚à¸£à¸‡à¸žà¸¢à¸²à¸šà¸²à¸¥',
            
            # Sports & fitness
            'à¸­à¸­à¸à¸à¸³à¸¥à¸±à¸‡à¸à¸²à¸¢', 'à¸Ÿà¸´à¸•à¹€à¸™à¸ª', 'à¸§à¸´à¹ˆà¸‡', 'à¸§à¹ˆà¸²à¸¢à¸™à¹‰à¸³', 'à¹‚à¸¢à¸„à¸°',
        ]
        
        for keyword in activity_keywords:
            if keyword in normalized_text:
                slots['description'] = keyword
                break
    
    # STEP 5: Pattern-based PERSON detection
    if not slots['attendees']:
        found_names = []
        
        # Expanded Thai titles and roles
        titles = [
            'à¸£à¸¨\\.à¸”à¸£\\.', 'à¸£à¸¨\\.', 'à¸œà¸¨\\.à¸”à¸£\\.', 'à¸œà¸¨\\.', 'à¸”à¸£\\.', 'à¸žà¸\\.', 'à¸™à¸ž\\.',
            'à¸­à¸²à¸ˆà¸²à¸£à¸¢à¹Œ', 'à¸„à¸¸à¸“', 'à¸™à¸²à¸¢', 'à¸™à¸²à¸‡à¸ªà¸²à¸§', 'à¸™à¸²à¸‡', 'à¸™\\.à¸ª\\.', 
            'à¸—à¹ˆà¸²à¸™', 'à¸žà¸µà¹ˆ', 'à¹€à¸žà¸·à¹ˆà¸­à¸™'
        ]
        
        roles = [
            'à¸œà¸­\\.', 'à¸œà¸¹à¹‰à¸­à¸³à¸™à¸§à¸¢à¸à¸²à¸£', 'à¸›à¸£à¸°à¸˜à¸²à¸™', 'à¹€à¸¥à¸‚à¸²à¸™à¸¸à¸à¸²à¸£', 'à¸™à¸¨\\.', 'à¸™à¸±à¸à¸¨à¸¶à¸à¸©à¸²'
        ]
        
        person_patterns = [
            # Full names: FirstName LastName (both must be Thai, 2+ chars each)
            r'([à¸-à¸®]{2,15})\s+([à¸-à¸®]{2,20})(?=\s|$|à¸—à¸µà¹ˆ|à¸•à¸­à¸™|à¹€à¸§à¸¥à¸²)',
            
            # Title + Full Name (e.g., "à¸£à¸¨.à¸”à¸£. à¸¨à¸´à¸£à¸§à¸´à¸Šà¸à¹Œ")
            rf'(?:{"|".join(titles)})\s+([à¸-à¸®][à¸-à¸®à¸°-à¸¹à¹€-à¹„à¹Œà¹ˆà¹‰à¹Šà¹‹à¹]{{2,25}})(?:\s+([à¸-à¸®]{{2,20}}))?(?=\s|$|à¸—à¸µà¹ˆ|à¸•à¸­à¸™)',
            
            # Role + name (e.g., "à¸œà¸­. à¸ªà¸¡à¸Šà¸±à¸¢")
            rf'(?:{"|".join(roles)})\s+([à¸-à¸®][à¸-à¸®à¸°-à¸¹à¹€-à¹„à¹Œà¹ˆà¹‰à¹Šà¹‹à¹]{{2,20}})(?=\s|$|à¸—à¸µà¹ˆ)',
            
            # "à¸à¸±à¸š" + name/nickname
            r'à¸à¸±à¸š\s+([à¸-à¸®][à¸-à¸®à¸°-à¸¹à¹€-à¹„à¹Œà¹ˆà¹‰à¹Šà¹‹à¹]{1,20})(?=\s|$|à¸—à¸µà¹ˆ|à¹à¸¥à¸°)',
            
            # Verbs + person (à¸žà¸š, à¹€à¸ˆà¸­, à¸™à¸±à¸”, etc.)
            r'(?:à¸žà¸š|à¹€à¸ˆà¸­|à¸™à¸±à¸”|à¸«à¸²|à¸•à¸´à¸”à¸•à¹ˆà¸­)\s+([à¸-à¸®][à¸-à¸®à¸°-à¸¹à¹€-à¹„à¹Œà¹ˆà¹‰à¹Šà¹‹à¹]{1,20})(?=\s|$|à¸—à¸µà¹ˆ)',
            
            # Group/department descriptors (e.g., "à¸­à¸²à¸ˆà¸²à¸£à¸¢à¹Œà¸ªà¸²à¸‚à¸²à¸§à¸´à¸Šà¸²à¸§à¸´à¸—à¸¢à¸²à¸à¸²à¸£à¸„à¸­à¸¡à¸¯")
            r'(à¸­à¸²à¸ˆà¸²à¸£à¸¢à¹Œ(?:à¸ªà¸²à¸‚à¸²)?(?:à¸§à¸´à¸Šà¸²)?[à¸-à¸®à¸°-à¸¹à¹€-à¹„à¹Œà¹ˆà¹‰à¹Šà¹‹à¹à¸¯\s]{3,40})(?=\s|$|à¸—à¸µà¹ˆ|à¸•à¸­à¸™|à¹€à¸§à¸¥à¸²|à¸§à¸±à¸™)',
            r'(à¸™à¸±à¸à¸¨à¸¶à¸à¸©à¸²[à¸-à¸®à¸°-à¸¹à¹€-à¹„à¹Œà¹ˆà¹‰à¹Šà¹‹à¹\s]{0,20})(?=\s|$|à¸—à¸µà¹ˆ)',
        ]
        
        for pattern in person_patterns:
            matches = re.findall(pattern, normalized_text)
            for match in matches:
                if isinstance(match, tuple):
                    # Handle captured groups (e.g., first name + last name)
                    name_parts = [m.strip() for m in match if m and m.strip()]
                    if name_parts:
                        found_names.extend(name_parts)
                else:
                    found_names.append(match.strip())
        
        # Exclusion filters
        excluded = {
            'à¸§à¸±à¸™', 'à¹€à¸§à¸¥à¸²', 'à¸—à¸µà¹ˆ', 'à¸•à¸­à¸™', 'à¹€à¸”à¸·à¸­à¸™', 'à¸›à¸µ', 'à¸›à¸£à¸°à¸Šà¸¸à¸¡',
            'à¸¡.à¸„.', 'à¸.à¸ž.', 'à¸¡à¸µ.à¸„.', 'à¹€à¸¡.à¸¢.', 'à¸ž.à¸„.', 'à¸¡à¸´.à¸¢.',
            'à¸.à¸„.', 'à¸ª.à¸„.', 'à¸.à¸¢.', 'à¸•.à¸„.', 'à¸ž.à¸¢.', 'à¸˜.à¸„.',
        }
        
        if found_names:
            # Filter and validate names
            names = [
                m.strip() for m in found_names
                if m.strip() not in excluded
                and len(m.strip()) >= 2  # Min 2 chars
                and len(m.strip()) <= 40  # Max 40 chars (for group names)
                and not m.strip()[0] in ['à¹Œ', 'à¸´', 'à¸µ', 'à¸¶', 'à¸·', 'à¸¸', 'à¸¹', 'à¹ˆ', 'à¹‰', 'à¹Š', 'à¹‹']
                and '.' not in m  # Exclude abbreviations with dots
            ]
            # Remove duplicates while preserving order
            seen = set()
            unique_names = []
            for name in names:
                if name not in seen and name:  # Also check not empty
                    seen.add(name)
                    unique_names.append(name)
            
            if unique_names:
                slots['attendees'] = ', '.join(unique_names[:2])  # Max 2 names
    
    # STEP 5.5: Pattern-based GENERIC PERSON detection (if no specific names found)
    if not slots['attendees']:
        generic_people = []
        
        # Pattern 1: "à¸à¸±à¸š" + generic person term
        generic_person_pattern = r'à¸à¸±à¸š\s*(à¹€à¸žà¸·à¹ˆà¸­à¸™|à¹à¸Ÿà¸™|à¸žà¸µà¹ˆ|à¸™à¹‰à¸­à¸‡|à¸žà¹ˆà¸­|à¹à¸¡à¹ˆ|à¸¥à¸¹à¸|à¸ªà¸²à¸¡à¸µ|à¸ à¸£à¸£à¸¢à¸²|à¹€à¸ˆà¹‰à¸²à¸™à¸²à¸¢|à¸«à¸±à¸§à¸«à¸™à¹‰à¸²|à¸—à¸µà¸¡|à¹€à¸žà¸·à¹ˆà¸­à¸™à¸£à¹ˆà¸§à¸¡à¸‡à¸²à¸™|à¸„à¸™à¸£à¸±à¸|à¹à¸Ÿà¸™à¸ªà¸²à¸§|à¹à¸Ÿà¸™à¸«à¸™à¸¸à¹ˆà¸¡)'
        matches = re.findall(generic_person_pattern, normalized_text)
        generic_people.extend(matches)
        
        # Pattern 2: generic person + action verbs (à¸žà¸š, à¹€à¸ˆà¸­, etc.)
        person_action_pattern = r'(à¹€à¸žà¸·à¹ˆà¸­à¸™|à¹à¸Ÿà¸™|à¸žà¸µà¹ˆ|à¸™à¹‰à¸­à¸‡)\s*(?:à¹„à¸›|à¸¡à¸²|à¸žà¸š|à¹€à¸ˆà¸­|à¸™à¸±à¸”)'
        matches = re.findall(person_action_pattern, normalized_text)
        generic_people.extend(matches)
        
        if generic_people:
            # Remove duplicates while preserving order
            unique_people = list(dict.fromkeys(generic_people))
            slots['attendees'] = ', '.join(unique_people[:2])  # Max 2
    
    # STEP 6: Pattern-based LOCATION detection  
    if not slots['location']:
        # Common location keywords - match more conservatively
        location_keywords = [
            'à¸•à¸¶à¸', 'à¸­à¸²à¸„à¸²à¸£', 'à¸«à¹‰à¸­à¸‡', 'à¸Šà¸±à¹‰à¸™', 'à¸¥à¸²à¸™',
            'à¹‚à¸£à¸‡à¸žà¸¢à¸²à¸šà¸²à¸¥', 'à¹‚à¸£à¸‡à¹€à¸£à¸µà¸¢à¸™', 'à¸¡à¸«à¸²à¸§à¸´à¸—à¸¢à¸²à¸¥à¸±à¸¢',
            'à¸¨à¸¹à¸™à¸¢à¹Œ', 'à¸„à¸“à¸°', 'à¸ªà¸³à¸™à¸±à¸à¸‡à¸²à¸™'
        ]
        
        for keyword in location_keywords:
            pattern = keyword + r'\s*([à¸-à¸®à¸²-à¸¹à¹€-à¹„0-9\s]{0,20})(?:\s|à¸—à¸µà¹ˆ|à¸•à¸­à¸™|à¹€à¸§à¸¥à¸²|$)'
            match = re.search(pattern, normalized_text)
            if match:
                # Preserve spacing between keyword and content
                content = match.group(1).strip()
                if content:
                    # Add space between keyword and number if missing
                    if content and content[0].isdigit():
                        location = keyword + ' ' + content
                    else:
                        location = keyword + content
                else:
                    location = keyword
                
                # Validate: should be 3-30 chars and not just the keyword
                if 3 <= len(location) <= 30 and location != keyword:
                    slots['location'] = location[:30]
                    break
        
        # Specific location patterns
        if not slots['location']:
            location_patterns = [
                (r'à¸—à¸µà¹ˆ\s*([à¸-à¸®][à¸-à¸®à¸²-à¸¹à¹€-à¹„\s]{2,25})(?:à¸•à¸­à¸™|à¹€à¸§à¸¥à¸²|à¸Šà¸±à¹‰à¸™|$)', 1),  # "à¸—à¸µà¹ˆ" + location
                (r'(zoom|google\s*meet|teams|online)', 0),  # Online platforms
            ]
            
            for pattern, group_idx in location_patterns:
                match = re.search(pattern, normalized_text, re.IGNORECASE)
                if match:
                    location_text = match.group(group_idx) if group_idx > 0 else match.group()
                    if any(word in location_text.lower() for word in ['zoom', 'meet', 'teams', 'online']):
                        slots['location'] = 'à¸­à¸­à¸™à¹„à¸¥à¸™à¹Œ'
                    else:
                        slots['location'] = location_text.strip()[:30]
                    break
    
    # Convert attendees list to string (only if it's a list)
    if slots['attendees']:
        if isinstance(slots['attendees'], list):
            slots['attendees'] = ', '.join(slots['attendees'])
        # If it's already a string (from generic person detection), leave it as is
    else:
        slots['attendees'] = None
    
    return slots


def create_event(slots: Dict[str, any], event_id: Optional[str] = None) -> Dict:
    """Create a structured event from slots"""
    if event_id is None:
        event_id = f"evt_{uuid.uuid4().hex[:8]}"
    
    event = {
        'id': event_id,
        'date': slots.get('date'),
        'time': slots.get('time'),
        'description': slots.get('description'),
        'attendees': slots.get('attendees'),
        'location': slots.get('location'),
        'raw_text': slots.get('raw_text', ''),
        'created_at': get_current_datetime().isoformat()
    }
    
    return event


def load_events(filepath: str = EVENTS_FILE) -> List[Dict]:
    """Load events from JSON file"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('events', [])
    except (FileNotFoundError, json.JSONDecodeError):
        return []


def save_events(events: List[Dict], filepath: str = EVENTS_FILE):
    """Save events to JSON file"""
    print(f"DEBUG: Saving {len(events)} events to {filepath}")  # Debug
    try:
        data = {'events': events}
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"DEBUG: Successfully saved to {filepath}")  # Debug
    except Exception as e:
        print(f"DEBUG: Error saving events: {e}")  # Debug
        raise


def add_event(event: Dict, filepath: str = EVENTS_FILE):
    """Add a new event to the file"""
    events = load_events(filepath)
    events.append(event)
    save_events(events, filepath)
    return event


def delete_event(event_id: str, filepath: str = EVENTS_FILE):
    """Delete an event by ID"""
    events = load_events(filepath)
    events = [e for e in events if e['id'] != event_id]
    save_events(events, filepath)


def update_event(event_id: str, updated_data: Dict, filepath: str = EVENTS_FILE):
    """
    Update an existing event by ID
    
    Args:
        event_id: ID of event to update
        updated_data: Dictionary with updated fields
        filepath: Path to events file
    """
    events = load_events(filepath)
    for i, event in enumerate(events):
        if event['id'] == event_id:
            # Preserve original metadata
            original_created_at = event.get('created_at')
            original_raw_text = event.get('raw_text')
            
            # Update fields
            events[i].update(updated_data)
            
            # Ensure critical fields are preserved
            events[i]['id'] = event_id
            if original_created_at:
                events[i]['created_at'] = original_created_at
            if original_raw_text:
                events[i]['raw_text'] = original_raw_text
            events[i]['updated_at'] = get_current_datetime().isoformat()
            
            save_events(events, filepath)
            return events[i]
    return None


def process_text_to_event(text: str, nlp_model=None, save_to_file: bool = False) -> Dict:
    """
    Complete pipeline: text â†’ slots â†’ validation â†’ event
    
    Returns event dict with additional validation metadata:
    - 'is_valid': bool
    - 'missing_fields': list of critical missing fields
    - 'auto_filled': dict of fields that were auto-filled
    """
    from validation import validate_event_data, apply_safe_defaults
    
    # Extract slots
    slots = extract_slots(text, nlp_model)
    
    # Validate and get safe defaults
    is_valid, missing_fields, safe_defaults = validate_event_data(slots)
    
    # Apply safe defaults
    slots_with_defaults = apply_safe_defaults(slots, safe_defaults)
    
    # Create event (without saving yet)
    event = create_event(slots_with_defaults)
    
    # Add validation metadata
    event['is_valid'] = is_valid
    event['missing_fields'] = missing_fields
    event['auto_filled'] = safe_defaults
    
    # Only save if explicitly requested AND validation passes
    if save_to_file and is_valid:
        add_event(event)
    
    return event
